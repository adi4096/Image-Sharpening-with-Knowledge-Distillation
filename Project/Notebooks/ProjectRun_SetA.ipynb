{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e25b27",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec025837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, shutil, torch, cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c0375",
   "metadata": {},
   "source": [
    "# Image Filtering & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed1452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Filtering images divisible by 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1254/1254 [00:09<00:00, 131.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Valid images: 989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating patches for train_A: 100%|██████████| 395/395 [00:13<00:00, 28.27it/s]\n",
      " Creating patches for train_B: 100%|██████████| 396/396 [00:13<00:00, 28.33it/s]\n",
      " Copying full images to test: 100%|██████████| 198/198 [00:00<00:00, 1240.56it/s]\n",
      " Copying full images to showcase: 100%|██████████| 50/50 [00:00<00:00, 1218.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset Preparation Complete:\n",
      " Train A: 4740 patches\n",
      " Train B: 4752 patches\n",
      " Test: 198 full images\n",
      " Showcase: 50 full images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "source_dir = r\"C:\\Users\\atole\\OneDrive\\Desktop\\Python\\archive(1)\\Image Super Resolution - Unsplash\\high res\"        \n",
    "target_root = r\"C:\\Users\\atole\\OneDrive\\Desktop\\Python\\Working Dataset\"\n",
    "os.makedirs(target_root, exist_ok=True)\n",
    "valid_images = []\n",
    "\n",
    "# This is to filter images to standard resolution i.e. in our case 1200*800\n",
    "print(\" Filtering images divisible by 8...\")\n",
    "for fname in tqdm(os.listdir(source_dir)): \n",
    "    if fname.lower().endswith(('.jpg', '.png')):\n",
    "        fpath = os.path.join(source_dir, fname)\n",
    "        img = cv2.imread(fpath)\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "        if h % 8 == 0 and w % 8 == 0:\n",
    "            valid_images.append(fname)\n",
    "print(f\" Valid images: {len(valid_images)}\")\n",
    "\n",
    "random.seed(42)\n",
    "train_imgs, test_imgs = train_test_split(valid_images, test_size=0.2, random_state=42)\n",
    "train_A, train_B = train_test_split(train_imgs, test_size=0.5, random_state=42)\n",
    "showcase_imgs = random.sample(test_imgs, min(50, len(test_imgs)))\n",
    "\n",
    "#For getting a wider variety of images for training we are breaking down the images into patches\n",
    "def extract_patches(img, patch_size=256, stride=256):\n",
    "    patches = []\n",
    "    h, w = img.shape[:2]\n",
    "    for y in range(0, h - patch_size + 1, stride):\n",
    "        for x in range(0, w - patch_size + 1, stride):\n",
    "            patch = img[y:y+patch_size, x:x+patch_size]\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "def save_patches_from_list(file_list, folder_name, patch_size=256, stride=256):\n",
    "    dest = os.path.join(target_root, folder_name)\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "\n",
    "    patch_id = 0\n",
    "    for fname in tqdm(file_list, desc=f\" Creating patches for {folder_name}\"):\n",
    "        img = cv2.imread(os.path.join(source_dir, fname))\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        h, w = h - (h % 8), w - (w % 8)\n",
    "        img = cv2.resize(img, (w, h))\n",
    "        patches = extract_patches(img, patch_size=patch_size, stride=stride)\n",
    "        for patch in patches:\n",
    "            out_path = os.path.join(dest, f\"{patch_id:06}.png\")\n",
    "            cv2.imwrite(out_path, cv2.cvtColor(patch, cv2.COLOR_RGB2BGR))\n",
    "            patch_id += 1\n",
    "\n",
    "def copy_images(file_list, folder_name):\n",
    "    dest = os.path.join(target_root, folder_name)\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "\n",
    "    for fname in tqdm(file_list, desc=f\" Copying full images to {folder_name}\"):\n",
    "        src_path = os.path.join(source_dir, fname)\n",
    "        dst_path = os.path.join(dest, fname)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "            \n",
    "save_patches_from_list(train_A, \"train_A\", patch_size=256, stride=256)   \n",
    "save_patches_from_list(train_B, \"train_B\", patch_size=256, stride=256)\n",
    "copy_images(test_imgs, \"test\")\n",
    "copy_images(showcase_imgs, \"showcase\")\n",
    "\n",
    "print(\"\\n Dataset Preparation Complete:\")\n",
    "print(f\" Train A: {len(os.listdir(os.path.join(target_root, 'train_A')))} patches\")\n",
    "print(f\" Train B: {len(os.listdir(os.path.join(target_root, 'train_B')))} patches\")\n",
    "print(f\" Test: {len(os.listdir(os.path.join(target_root, 'test')))} full images\")\n",
    "print(f\" Showcase: {len(os.listdir(os.path.join(target_root, 'showcase')))} full images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f70a3",
   "metadata": {},
   "source": [
    "# Importing restormer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3202fa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to restormer_arch.py\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/swz30/Restormer/main/basicsr/models/archs/restormer_arch.py\"\n",
    "save_path = \"restormer_arch.py\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Saved to {save_path}\")\n",
    "else:\n",
    "    print(f\" Failed to download. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ec1fe",
   "metadata": {},
   "source": [
    "# Loading Restormer with its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "080747b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\atole\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.8.1)\n",
      " Restormer (3-channel teacher for single-image defocus deblurring) ready\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!pip install einops\n",
    "sys.path.append(r'C:\\Users\\atole\\OneDrive\\Desktop\\Python\\Working Dataset')\n",
    "from restormer_arch import Restormer\n",
    "import torch\n",
    "\n",
    "teacher = Restormer(\n",
    "    inp_channels=3,               \n",
    "    out_channels=3,\n",
    "    dim=48,\n",
    "    num_blocks=[4, 6, 6, 8],\n",
    "    num_refinement_blocks=4,\n",
    "    heads=[1, 2, 4, 8],\n",
    "    ffn_expansion_factor=2.66,\n",
    "    bias=False,\n",
    "    LayerNorm_type='WithBias',\n",
    "    dual_pixel_task=False        \n",
    ")\n",
    "\n",
    "\n",
    "weights = torch.load(r\"C:\\Users\\atole\\OneDrive\\Desktop\\Python\\single_image_defocus_deblurring.pth\", map_location=device)\n",
    "teacher.load_state_dict(weights.get(\"params\", weights))\n",
    "teacher = teacher.to(device).eval()\n",
    "\n",
    "print(\" Restormer (3-channel teacher for single-image defocus deblurring) ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336d3a3",
   "metadata": {},
   "source": [
    "# Defining the student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d976ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, 3, padding=1), torch.nn.ReLU(), #RGB → 32 feature maps\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1), torch.nn.ReLU(), #32 → 64 feature maps\n",
    "            torch.nn.MaxPool2d(2), # Downsample by 2 \n",
    "            torch.nn.Conv2d(64, 64, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) # Upsample back to original size\n",
    "        )\n",
    "        #Decoder: To reconstruct the output\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 32, 3, padding=1), torch.nn.ReLU(), # 64 → 32 feature maps\n",
    "            torch.nn.Conv2d(32, 3, 3, padding=1), torch.nn.Tanh()  # 32 → RGB output, Output in range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x) # Summarize: Feature extraction and size restoration\n",
    "        x = self.decoder(x) # Decoding Image back to RGB\n",
    "        return (x + 1) / 2  # Output in range [0, 1]\n",
    "\n",
    "student = StudentCNN().to(device)\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-3) # Adam optimizer with learning rate 0.001\n",
    "criterion = torch.nn.L1Loss() # Use L1 loss (pixel-wise difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57045a0c",
   "metadata": {},
   "source": [
    "# Perceptual Loss Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9845fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atole\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\atole\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\atole/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528M/528M [03:16<00:00, 2.82MB/s] \n"
     ]
    }
   ],
   "source": [
    "vgg = vgg16(pretrained=True).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg_normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def perceptual_loss(pred, target):\n",
    "    pred_resized = F.interpolate(pred, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    target_resized = F.interpolate(target, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    pred_norm = vgg_normalize(pred_resized.squeeze(0)).unsqueeze(0)\n",
    "    target_norm = vgg_normalize(target_resized.squeeze(0)).unsqueeze(0)\n",
    "    pred_feat = vgg(pred_norm)\n",
    "    target_feat = vgg(target_norm)\n",
    "    return F.l1_loss(pred_feat, target_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4aa648",
   "metadata": {},
   "source": [
    "# Training Loop for Set A of images & Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 1: 100%|██████████| 4740/4740 [28:35<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1 | Avg L1 Loss: 0.077644 | Avg SSIM: 0.7805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 2: 100%|██████████| 4740/4740 [28:52<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2 | Avg L1 Loss: 0.059260 | Avg SSIM: 0.8404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 3: 100%|██████████| 4740/4740 [28:47<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3 | Avg L1 Loss: 0.044978 | Avg SSIM: 0.8674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 4: 100%|██████████| 4740/4740 [28:18<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 4 | Avg L1 Loss: 0.041758 | Avg SSIM: 0.8753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 5: 100%|██████████| 4740/4740 [28:39<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 5 | Avg L1 Loss: 0.039660 | Avg SSIM: 0.8830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 6: 100%|██████████| 4740/4740 [28:30<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 6 | Avg L1 Loss: 0.039505 | Avg SSIM: 0.8864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 7: 100%|██████████| 4740/4740 [28:42<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 7 | Avg L1 Loss: 0.039845 | Avg SSIM: 0.8862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 8: 100%|██████████| 4740/4740 [28:39<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 8 | Avg L1 Loss: 0.040055 | Avg SSIM: 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 9: 100%|██████████| 4740/4740 [28:01<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 9 | Avg L1 Loss: 0.040332 | Avg SSIM: 0.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 10: 100%|██████████| 4740/4740 [28:12<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 10 | Avg L1 Loss: 0.038801 | Avg SSIM: 0.8907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 11: 100%|██████████| 4740/4740 [28:10<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 11 | Avg L1 Loss: 0.038823 | Avg SSIM: 0.8892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 12: 100%|██████████| 4740/4740 [28:07<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 12 | Avg L1 Loss: 0.039860 | Avg SSIM: 0.8884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 13: 100%|██████████| 4740/4740 [29:05<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 13 | Avg L1 Loss: 0.038409 | Avg SSIM: 0.8895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 14: 100%|██████████| 4740/4740 [28:12<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 14 | Avg L1 Loss: 0.038985 | Avg SSIM: 0.8930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training Student | Epoch 15: 100%|██████████| 4740/4740 [28:20<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 15 | Avg L1 Loss: 0.038613 | Avg SSIM: 0.8933\n",
      " Best model saved at Epoch 15 with SSIM: 0.8933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(target_root, \"train_A\")  \n",
    "train_files = os.listdir(train_path)\n",
    "student.train()\n",
    "teacher.eval()\n",
    "best_ssim = 0.0\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    student.train()\n",
    "    ssim_total = 0.0\n",
    "\n",
    "    for fname in tqdm(train_files, desc=f\" Training Student | Epoch {epoch+1}\"):\n",
    "        img = cv2.imread(os.path.join(train_path, fname))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        h, w = h - (h % 8), w - (w % 8)\n",
    "        gt = cv2.resize(img, (w, h))\n",
    "        lr = cv2.resize(gt, (w//2, h//2), interpolation=cv2.INTER_LINEAR)\n",
    "        input_img = cv2.resize(lr, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        input_tensor = to_tensor(input_img).unsqueeze(0).to(device)\n",
    "        gt_tensor = to_tensor(gt).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_tensor = teacher(input_tensor)\n",
    "        output = student(input_tensor)\n",
    "        l1 = criterion(output, target_tensor)\n",
    "        gt_l1 = criterion(output, gt_tensor)\n",
    "        p_loss = perceptual_loss(output, gt_tensor)\n",
    "        loss = 0.9 * l1 + 0.1 * gt_l1 + 0.005 * p_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            s_output = output.squeeze(0).clamp(0, 1).cpu().numpy()\n",
    "            s_gt = to_tensor(gt).numpy()\n",
    "            s_output = np.transpose(s_output, (1, 2, 0))\n",
    "            s_gt = np.transpose(s_gt, (1, 2, 0))\n",
    "            score = ssim(s_output, s_gt, channel_axis=2, data_range=1.0, win_size=11)\n",
    "            ssim_total += score\n",
    "\n",
    "    avg_loss = total_loss / len(train_files)\n",
    "    avg_ssim = ssim_total / len(train_files)\n",
    "    print(f\" Epoch {epoch+1} | Avg L1 Loss: {avg_loss:.6f} | Avg SSIM: {avg_ssim:.4f}\")\n",
    "\n",
    "if avg_ssim > best_ssim:\n",
    "        best_ssim = avg_ssim\n",
    "        torch.save(student.state_dict(), \"best_student.pth\")\n",
    "        print(f\" Best model saved at Epoch {epoch+1} with SSIM: {best_ssim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
