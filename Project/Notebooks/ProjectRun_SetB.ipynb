{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb6e326",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, shutil, torch, cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9c7ee",
   "metadata": {},
   "source": [
    "# Setting Up Directories and Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = r\"C:\\Users\\atole\\OneDrive\\Desktop\\Python\\archive(1)\\Image Super Resolution - Unsplash\\high res\"\n",
    "target_root = r\"\\Users\\atole\\OneDrive\\Desktop\\Python\\Working Dataset\"\n",
    "os.makedirs(target_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f08d45",
   "metadata": {},
   "source": [
    "# Setting Up Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e243d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595451c2",
   "metadata": {},
   "source": [
    "# Importing Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/swz30/Restormer/main/basicsr/models/archs/restormer_arch.py\"\n",
    "save_path = \"restormer_arch.py\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Saved to {save_path}\")\n",
    "else:\n",
    "    print(f\" Failed to download. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44376eac",
   "metadata": {},
   "source": [
    "# Loading Restormer with its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aaf2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install einops\n",
    "sys.path.append(r'C:\\Users\\atole\\OneDrive\\Desktop\\Python\\Working Dataset')\n",
    "from restormer_arch import Restormer\n",
    "import torch\n",
    "\n",
    "teacher = Restormer(\n",
    "    inp_channels=3,               \n",
    "    out_channels=3,\n",
    "    dim=48,\n",
    "    num_blocks=[4, 6, 6, 8],\n",
    "    num_refinement_blocks=4,\n",
    "    heads=[1, 2, 4, 8],\n",
    "    ffn_expansion_factor=2.66,\n",
    "    bias=False,\n",
    "    LayerNorm_type='WithBias',\n",
    "    dual_pixel_task=False        \n",
    ")\n",
    "\n",
    "\n",
    "weights = torch.load(r\"C:\\Users\\atole\\OneDrive\\Desktop\\Python\\single_image_defocus_deblurring.pth\", map_location=device)\n",
    "teacher.load_state_dict(weights.get(\"params\", weights))\n",
    "teacher = teacher.to(device).eval()\n",
    "\n",
    "print(\" Restormer (3-channel teacher for single-image defocus deblurring) ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e0dce",
   "metadata": {},
   "source": [
    "# Defining the Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1098b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(64, 64, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 32, 3, padding=1), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 3, 3, padding=1), torch.nn.Tanh()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return (x + 1) / 2\n",
    "\n",
    "student = StudentCNN().to(device)\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaed34c",
   "metadata": {},
   "source": [
    "# Perceptual Loss Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = vgg16(pretrained=True).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg_normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def perceptual_loss(pred, target):\n",
    "    pred_resized = F.interpolate(pred, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    target_resized = F.interpolate(target, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "    pred_norm = vgg_normalize(pred_resized.squeeze(0)).unsqueeze(0)\n",
    "    target_norm = vgg_normalize(target_resized.squeeze(0)).unsqueeze(0)\n",
    "    pred_feat = vgg(pred_norm)\n",
    "    target_feat = vgg(target_norm)\n",
    "    return F.l1_loss(pred_feat, target_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea97fe",
   "metadata": {},
   "source": [
    "# Training loop for Set B of images & Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(target_root, \"train_B\")  \n",
    "train_files = os.listdir(train_path)\n",
    "student.train()\n",
    "teacher.eval()\n",
    "best_ssim = 0.0\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    student.train()\n",
    "    ssim_total = 0.0\n",
    "\n",
    "    for fname in tqdm(train_files, desc=f\" Training Student | Epoch {epoch+1}\"):\n",
    "        img = cv2.imread(os.path.join(train_path, fname))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        h, w = h - (h % 8), w - (w % 8)\n",
    "        gt = cv2.resize(img, (w, h))\n",
    "        lr = cv2.resize(gt, (w//2, h//2), interpolation=cv2.INTER_LINEAR)\n",
    "        input_img = cv2.resize(lr, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        input_tensor = to_tensor(input_img).unsqueeze(0).to(device)\n",
    "        gt_tensor = to_tensor(gt).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_tensor = teacher(input_tensor)\n",
    "\n",
    "        output = student(input_tensor)\n",
    "\n",
    "        l1 = criterion(output, target_tensor)\n",
    "        gt_l1 = criterion(output, gt_tensor)\n",
    "        p_loss = perceptual_loss(output, gt_tensor)\n",
    "        loss = 0.9 * l1 + 0.1 * gt_l1 + 0.005 * p_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            s_output = output.squeeze(0).clamp(0, 1).cpu().numpy()\n",
    "            s_gt = to_tensor(gt).numpy()\n",
    "            s_output = np.transpose(s_output, (1, 2, 0))\n",
    "            s_gt = np.transpose(s_gt, (1, 2, 0))\n",
    "            score = ssim(s_output, s_gt, channel_axis=2, data_range=1.0, win_size=11)\n",
    "            ssim_total += score\n",
    "\n",
    "    avg_loss = total_loss / len(train_files)\n",
    "    avg_ssim = ssim_total / len(train_files)\n",
    "    print(f\" Epoch {epoch+1} | Avg L1 Loss: {avg_loss:.6f} | Avg SSIM: {avg_ssim:.4f}\")\n",
    "\n",
    "if avg_ssim > best_ssim:\n",
    "        best_ssim = avg_ssim\n",
    "        torch.save(student.state_dict(), \"best_student.pth\")\n",
    "        print(f\" Best model saved at Epoch {epoch+1} with SSIM: {best_ssim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
